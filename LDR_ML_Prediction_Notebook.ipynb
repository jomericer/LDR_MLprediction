{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4dOg3kaTt0W1",
        "Q3MsqanytyWs",
        "WY_sf5BwioP7",
        "obve2xazirTo",
        "EEU6uyzPivV4",
        "dz0PASp3SWi_"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize"
      ],
      "metadata": {
        "id": "Wk8PWMg1KAtr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkJrPJjzJwYp"
      },
      "outputs": [],
      "source": [
        "! pip install shap\n",
        "\n",
        "import shap\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import xgboost as xgb \n",
        "import os\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split \n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc, precision_score, recall_score, f1_score, make_scorer, log_loss\n",
        "from sklearn.model_selection import cross_val_score, cross_validate, cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "from matplotlib.pylab import rcParams\n",
        "from matplotlib import pyplot\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "RAND_STATE = 30\n",
        "\n",
        "def Performance(Model,Y,X,Yt,Xt,title):\n",
        "    rcParams['figure.figsize'] = 4, 4\n",
        "    # Perforamnce of the model\n",
        "    fpr, tpr, _ = roc_curve(Y, Model.predict_proba(X)[:,1])\n",
        "    AUC  = metrics.auc(fpr, tpr)\n",
        "    fprt, tprt, _ = roc_curve(Yt, Model.predict_proba(Xt)[:,1])\n",
        "    AUCt  = metrics.auc(fprt, tprt)\n",
        "    plt.figure()\n",
        "    plt.plot(fprt, tprt, label='AUC Train = %0.4f' % AUCt)\n",
        "    plt.plot(fpr, tpr, label='AUC Test = %0.4f' % AUC)\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('1 - Specificity')\n",
        "    plt.ylabel('Sensitivity')\n",
        "    plt.title(title)\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# define a function which will help us create XGBoost models and perform cross-validation\n",
        "def modelfit(alg, dtrain, dtarget, xtest, ytest, cv_folds, estn, useTrainCV=True, early_stopping_rounds=50):\n",
        "    predictors = [x for x in dtrain.columns]\n",
        "    if useTrainCV:\n",
        "        cv = StratifiedKFold(n_splits=cv_folds)\n",
        "        xgb_param = alg.get_xgb_params()\n",
        "        print(f'Using CV with {cv_folds} folds')\n",
        "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtarget.values)\n",
        "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], folds=cv,\n",
        "            metrics=['logloss','auc'], early_stopping_rounds=early_stopping_rounds)\n",
        "        print(\"CV results:\")\n",
        "        print(f\"  Train logloss = {np.mean(cvresult['train-logloss-mean']):.3f} ({np.mean(cvresult['train-logloss-std']):.3f})\" \\\n",
        "              f\"  Train AUC = {np.mean(cvresult['train-auc-mean']):.3f} ({np.mean(cvresult['train-auc-std']):.3f})\")\n",
        "        print(f\"  Test logloss = {np.mean(cvresult['test-logloss-mean']):.3f} ({np.mean(cvresult['test-logloss-std']):.3f})\" \\\n",
        "              f\"  Test AUC = {np.mean(cvresult['test-auc-mean']):.3f} ({np.mean(cvresult['test-auc-std']):.3f})\")\n",
        "        if estn == 0:\n",
        "          print(f\"\\nOptimum trees using XGB CV={cvresult.shape[0]}\")\n",
        "          alg.set_params(n_estimators=cvresult.shape[0]) # check the optimum number of trees using cv function of xgboost\n",
        "        else:\n",
        "          print(f\"\\nOptimum trees using XGB CV={cvresult.shape[0]}, currently set to {estn}\")\n",
        "          alg.set_params(n_estimators=estn)\n",
        " \n",
        "    #Fit the algorithm on the data\n",
        "    print('fitting...')\n",
        "    alg.fit(dtrain[predictors], dtarget.values.ravel(), eval_set=[(dtrain, dtarget.values.ravel()), (xtest, ytest.values.ravel())], \n",
        "            eval_metric=['logloss'], early_stopping_rounds=early_stopping_rounds, verbose=0)\n",
        "    print(f'Early stopping: best (score, iter, ntree limit) = {alg.best_score, alg.best_iteration, alg.best_ntree_limit}') \n",
        "\n",
        "    #Predict training set:\n",
        "    dtrain_predictions = alg.predict(dtrain[predictors]) # i.e.: y_pred\n",
        "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1] # i.e.: y_score\n",
        "    dtest_predict = alg.predict(xtest)\n",
        "    dtest_predprob = alg.predict_proba(xtest)[:,1]\n",
        "        \n",
        "    #Print model report:\n",
        "    print(f\"\\n{'MODEL REPORT':26} {'train':10} | {'test':10}\")\n",
        "    print(f\" {'Accuracy:':25} {metrics.accuracy_score(dtarget.values, dtrain_predictions):10.4f} | \" \\\n",
        "          f\"{metrics.accuracy_score(ytest.values, dtest_predict):10.4f}\")\n",
        "    print(f\" {'Precision Score:':25} {metrics.precision_score(dtarget.values, dtrain_predictions):10.4f} | \" \\\n",
        "          f\"{metrics.precision_score(ytest.values, dtest_predict):10.4f}\")\n",
        "    print(f\" {'Recall (sensitivity):':25} {metrics.recall_score(dtarget, dtrain_predictions):10.4f} | \" \\\n",
        "          f\"{metrics.recall_score(ytest, dtest_predict):10.4f}\")\n",
        "    print(f\" {'F1 Score:':25} {metrics.f1_score(dtarget.values, dtrain_predictions):10.4f} | \" \\\n",
        "          f\"{metrics.f1_score(ytest.values, dtest_predict):10.4f}\")\n",
        "    print(f\" {'AUC Score:':25} {metrics.roc_auc_score(dtarget, dtrain_predprob):10.4f} | \" \\\n",
        "          f\"{metrics.roc_auc_score(ytest, dtest_predprob):10.4f}\")\n",
        "\n",
        "    rcParams['figure.figsize'] = 12, 4\n",
        "    feat_imp = pd.Series(alg.get_booster().get_fscore()).sort_values(ascending=False)\n",
        "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
        "    plt.ylabel('Feature Importance Score')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agricultural-computer"
      },
      "source": [
        "# Data handling\n",
        "load both databases, format (including one-hot encoding where appropriate), and merge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlmLuLlnuBf5"
      },
      "source": [
        "df = pd.DataFrame(data=[]) # reset df\n",
        "\n",
        "# fix eicu \n",
        "df = pd.read_csv('/eICU_all_ml.csv')\n",
        "df = pd.concat([df.loc[:,:'currICUDay'],\n",
        "                df.loc[:,'dr_uo'],\n",
        "                df.loc[:,'age':'ethnicity'],\n",
        "                df.loc[:,'congestive_heart_failure':]], axis=1)\n",
        "\n",
        "df = df.loc[df['dr_uo'].notnull()].copy()\n",
        "df['ethnicity'].replace({'African American':'Other', 'Hispanic':'Other', 'Asian':'Other', 'Native American':'Other',\n",
        "                         'Other/Unknown':'Other'},inplace=True)\n",
        "df.rename(columns={\"ethnicity\": \"ethn\", \"gender\": \"sex\"}, inplace=True)\n",
        "df = pd.get_dummies(df, columns=['sex','ethn'])\n",
        "print('eicu data formatted')\n",
        "\n",
        "# fix mimic\n",
        "df_ = pd.read_csv('/MIMIC_all_noimpute.csv')\n",
        "df_ = df_[df_['dr_uo'].isnull()==False]\n",
        "df_ = pd.concat([df_.loc[:,'dr_uo'],\n",
        "                df_.loc[:,'age':'ethn'],\n",
        "                df_.loc[:,'congestive_heart_failure':'depression'], \n",
        "                df_.loc[:,'s_Na':]], axis=1) # gender + ethn + elixhauser + other diuretics + DDI + labs\n",
        "\n",
        "df_['sex'].replace({1: 'female', 0: 'male'},inplace=True)\n",
        "df_['ethn'].replace({0: 'Caucasian', 1: 'Other', 2: 'Other', 3:'Other', 4: 'Other', 5: 'Other', 6: 'Other'},inplace=True)\n",
        "\n",
        "df_ = pd.get_dummies(df_, columns=['ethn','sex'])\n",
        "print(\"finished mimic data prep\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dOg3kaTt0W1"
      },
      "source": [
        "# short Cols"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "comic-egypt"
      },
      "source": [
        "# select variables to be included in analysis (abbr)\n",
        "cols_1=['age', 'sex_female', 'ethn_Caucasian', 's_Cl', 's_K', 's_HCO3', 's_Cr', 's_Glu', 's_Ca', 's_Hct', 's_Plt', 's_WBC', 'avg_HR', 'avg_sBP', 'avg_RR', 'avg_TempC', 'avg_SpO2', 'congestive_heart_failure', 'diabetes_uncomplicated', 'renal_failure', 'liver_disease']\n",
        "cols_2=['age', 'sex_female', 'ethn_Caucasian', 's_Cl', 's_K', 's_HCO3', 's_Cr', 's_Glu', 's_Ca', 's_Hgb', 's_Plt', 's_WBC', 'avg_HR', 'avg_sBP', 'avg_RR', 'avg_TempC', 'avg_SpO2', 'congestive_heart_failure', 'diabetes_uncomplicated', 'renal_failure', 'liver_disease']\n",
        "print(\"columns complete\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08G8CtJtMDAk"
      },
      "source": [
        "# **Train/Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bored-blackberry"
      },
      "source": [
        "# generate train and test datasets\n",
        "# set test dataset to 20% of data-> 80/20 split = Pareto principle\n",
        "# for models 1, 2, 4\n",
        "\n",
        "X = df.loc[:,cols_1].copy()\n",
        "y = df['dr_uo'].copy()\n",
        "\n",
        "X__test = df_.loc[:,cols_1].copy()\n",
        "y__test = df_['dr_uo'].copy()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y) \n",
        "print(\"Train and Test datasets created for models 1, 2, and 4... XGBoost ready\")\n",
        "\n",
        "\n",
        "# for model 3\n",
        "\n",
        "X2 = df.loc[:,cols_2].copy()\n",
        "y2 = df['dr_uo'].copy()\n",
        "\n",
        "X2__test = df_.loc[:,cols_2].copy()\n",
        "y2__test = df_['dr_uo'].copy()\n",
        "\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.20, stratify=y2) # set test dataset to 20% of data-> 80/20 split = Pareto principle\n",
        "print(\"Train and Test datasets created for model 4... XGBoost ready\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3MsqanytyWs"
      },
      "source": [
        "# **Model 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "statewide-robinson"
      },
      "source": [
        "# build xgb models with hyperparameter settings\n",
        "\n",
        "n_est = 413 # set to 0 to allow CV to pick\n",
        "\n",
        "xgb_1 = xgb.XGBClassifier(objective='binary:logistic',\n",
        "                     max_depth = 9,\n",
        "                     min_child_weight = 5,\n",
        "                     gamma = 9,\n",
        "                     subsample = 0.9478124851219693,\n",
        "                     colsample_bytree = 0.8067607006519033, \n",
        "                     reg_alpha = 0.9946342157271248,\n",
        "                     reg_lambda = 1,\n",
        "                     learning_rate = 0.1,\n",
        "                     eval_metric='logloss',\n",
        "                     missing=np.nan,\n",
        "                     use_label_encoder=False) \n",
        "\n",
        "modelfit(xgb_1, X_train, y_train, X_test, y_test, 5, n_est) # last # = n_estimators, 2nd last = Kfolds for CV\n",
        "\n",
        "Performance(Model=xgb_1,Y=y_test,X=X_test,Yt=y_train,Xt=X_train,title=\"Model 1 ROC\")\n",
        "\n",
        "pars=xgb_1.get_xgb_params()\n",
        "print(\"\\nfinal tuning:\")\n",
        "print(f\"'max_depth':{pars['max_depth']}, 'min_child_weight':{pars['min_child_weight']}, 'gamma':{pars['gamma']}\" \\\n",
        "      f\", 'subsample':{pars['subsample']}, 'colsample_bytree':{pars['colsample_bytree']}, \" \\\n",
        "      f\" 'reg_alpha':{pars['reg_alpha']}, 'reg_lambda':{pars['reg_lambda']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model 2**"
      ],
      "metadata": {
        "id": "WY_sf5BwioP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_est = 304 # set to 0 to allow CV to pick\n",
        "\n",
        "xgb_2 = xgb.XGBClassifier(objective='binary:logistic',\n",
        "                     max_depth = 3,\n",
        "                     min_child_weight = 2,\n",
        "                     gamma = 4.516721893114058,\n",
        "                     subsample = 0.8469450038636233,\n",
        "                     colsample_bytree = 0.968174437977502, \n",
        "                     reg_alpha = 0.18678516647095939,\n",
        "                     reg_lambda = 1,\n",
        "                     learning_rate = 0.1,\n",
        "                     eval_metric='logloss', \n",
        "                     missing=np.nan,\n",
        "                     use_label_encoder=False) \n",
        "\n",
        "modelfit(xgb_2, X_train, y_train, X_test, y_test, 5, n_est) # last # = n_estimators, 2nd last = Kfolds for CV\n",
        "\n",
        "Performance(Model=xgb_2,Y=y_test,X=X_test,Yt=y_train,Xt=X_train,title=\"Model 2 ROC\")\n",
        "\n",
        "pars=xgb_2.get_xgb_params()\n",
        "print(\"\\nfinal tuning:\")\n",
        "print(f\"'max_depth':{pars['max_depth']}, 'min_child_weight':{pars['min_child_weight']}, 'gamma':{pars['gamma']}\" \\\n",
        "      f\", 'subsample':{pars['subsample']}, 'colsample_bytree':{pars['colsample_bytree']}, \" \\\n",
        "      f\" 'reg_alpha':{pars['reg_alpha']}, 'reg_lambda':{pars['reg_lambda']}\")"
      ],
      "metadata": {
        "id": "lZ2tc9RAiq9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model 3**"
      ],
      "metadata": {
        "id": "obve2xazirTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_est = 157 # set to 0 to allow CV to pick\n",
        "\n",
        "xgb__ran = xgb.XGBClassifier(objective='binary:logistic',\n",
        "                     max_depth = 3,\n",
        "                     min_child_weight = 1,\n",
        "                     gamma = 4.316,\n",
        "                     subsample = 0.8161,\n",
        "                     colsample_bytree = 0.937, \n",
        "                     reg_alpha = 0.3988,\n",
        "                     reg_lambda = 1,\n",
        "                     learning_rate = 0.1,\n",
        "                     eval_metric='logloss', \n",
        "                     missing=np.nan,\n",
        "                     use_label_encoder=False) \n",
        "\n",
        "modelfit(xgb_3, X2_train, y2_train, X2_test, y2_test, 5, n_est) # last # = n_estimators, 2nd last = Kfolds for CV\n",
        "\n",
        "Performance(Model=xgb_3,Y=y2_test,X=X2_test,Yt=y2_train,Xt=X2_train,title=\"Model 3 ROC\")\n",
        "\n",
        "pars=xgb_3.get_xgb_params()\n",
        "print(\"\\nfinal tuning:\")\n",
        "print(f\"'max_depth':{pars['max_depth']}, 'min_child_weight':{pars['min_child_weight']}, 'gamma':{pars['gamma']}\" \\\n",
        "      f\", 'subsample':{pars['subsample']}, 'colsample_bytree':{pars['colsample_bytree']}, \" \\\n",
        "      f\" 'reg_alpha':{pars['reg_alpha']}, 'reg_lambda':{pars['reg_lambda']}\")"
      ],
      "metadata": {
        "id": "WRUwRB0Aiu7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model 4**"
      ],
      "metadata": {
        "id": "EEU6uyzPivV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_est = 196 # set to 0 to allow CV to pick\n",
        "\n",
        "xgb__roc = xgb.XGBClassifier(objective='binary:logistic',\n",
        "                     max_depth = 9,\n",
        "                     min_child_weight = 1,\n",
        "                     gamma = 4.474622122766906,\n",
        "                     subsample = 0.9880695217296125,\n",
        "                     colsample_bytree = 0.8577812370816866, \n",
        "                     reg_alpha = 7.919701369390835,\n",
        "                     reg_lambda = 1,\n",
        "                     learning_rate = 0.1,\n",
        "                     eval_metric='logloss', \n",
        "                     missing=np.nan,\n",
        "                     use_label_encoder=False) \n",
        "\n",
        "modelfit(xgb_4, X_train, y_train, X_test, y_test, 5, n_est) # last # = n_estimators, 2nd last = Kfolds for CV\n",
        "\n",
        "Performance(Model=xgb_4,Y=y_test,X=X_test,Yt=y_train,Xt=X_train,title=\"Model 4 ROC\")\n",
        "\n",
        "pars=xgb_4.get_xgb_params()\n",
        "print(\"\\nfinal tuning:\")\n",
        "print(f\"'max_depth':{pars['max_depth']}, 'min_child_weight':{pars['min_child_weight']}, 'gamma':{pars['gamma']}\" \\\n",
        "      f\", 'subsample':{pars['subsample']}, 'colsample_bytree':{pars['colsample_bytree']}, \" \\\n",
        "      f\" 'reg_alpha':{pars['reg_alpha']}, 'reg_lambda':{pars['reg_lambda']}\")"
      ],
      "metadata": {
        "id": "AUz9dsROp7Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k_MKGNRbSSdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SHAP, partial dependence"
      ],
      "metadata": {
        "id": "dz0PASp3SWi_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKfJXe0cRT7W"
      },
      "source": [
        "# set model\n",
        "X_model = xgb_1\n",
        "X_full = X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot feature importance plots of gain and weight\n",
        "\n",
        "xgb.plot_importance(X_model, title='Feature importance: Gain', grid=False,\n",
        "                    importance_type='gain', show_values=False)\n",
        "\n",
        "xgb.plot_importance(X_model, title='Feature importance: Weight', grid=False,\n",
        "                    importance_type='weight', show_values=False)"
      ],
      "metadata": {
        "id": "rr9DecVzk-hK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate SHAP summary plot\n",
        "\n",
        "explainer = shap.TreeExplainer(X_model)\n",
        "shap_values = explainer.shap_values(X_full)\n",
        "shap.summary_plot(shap_values, X_full, color='white')"
      ],
      "metadata": {
        "id": "VBD5_NqNs6V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate partial dependence plot of 4 most important variables\n",
        "\n",
        "pdp = PartialDependenceDisplay.from_estimator(X_model, X_full, [\"s_Cr\", \"avg_sBP\", \"s_Cl\", \"age\"], n_cols=4)"
      ],
      "metadata": {
        "id": "gQ7O8pHObqaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Ensemble Model***"
      ],
      "metadata": {
        "id": "JwY8LjVKTj0j"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnkH99bWyiTj"
      },
      "source": [
        "# create function to return probability based on threshold\n",
        "def adj_prob(y_pred, t):\n",
        "  if t == 0.5: \n",
        "      return [y_pred][0]\n",
        "  else: \n",
        "      return [0.4 if (y >= 0.5 and y < t) else y for y in y_pred]\n",
        "\n",
        "# set number of models\n",
        "num_models = 4.0 \n",
        "print(f'{num_models} models set')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwaPxZelE3ot"
      },
      "source": [
        "# train datasets\n",
        "ypred_xgb_1_train = adj_prob(xgb_1.predict_proba(X_train)[:,1], t=0.5)\n",
        "ypred_xgb_2_train = adj_prob(xgb_2.predict_proba(X_train)[:,1], t=0.5)\n",
        "ypred_xgb_3_train = adj_prob(xgb_3.predict_proba(X2_train)[:,1], t=0.5)\n",
        "ypred_xgb_4_train = adj_prob(xgb_4.predict_proba(X_train)[:,1], t=0.5)\n",
        "\n",
        "# test datasets\n",
        "ypred_xgb_1_test = adj_prob(xgb_1.predict_proba(X_test)[:,1], t=0.5)\n",
        "ypred_xgb_2_test = adj_prob(xgb_2.predict_proba(X_test)[:,1], t=0.5)\n",
        "ypred_xgb_3_test = adj_prob(xgb_3.predict_proba(X2_test)[:,1], t=0.5)\n",
        "ypred_xgb_4_test = adj_prob(xgb_4.predict_proba(X_test)[:,1], t=0.5)\n",
        "\n",
        "# validate dataset\n",
        "ypred_xgb_1_val = adj_prob(xgb_1.predict_proba(X__test)[:,1], t=0.5)\n",
        "ypred_xgb_2_val = adj_prob(xgb_2.predict_proba(X__test)[:,1], t=0.5)\n",
        "ypred_xgb_3_val = adj_prob(xgb_3.predict_proba(X2__test)[:,1], t=0.5)\n",
        "ypred_xgb_4_val = adj_prob(xgb_4.predict_proba(X__test)[:,1], t=0.5)\n",
        "\n",
        "\n",
        "# geometric means\n",
        "ypredprob_geo_mean_train = [(aa*bb*cc*dd)**(1/num_models) for aa, bb, cc, dd in zip(ypred_xgb_1_train, ypred_xgb_2_train, ypred_xgb_3_train, ypred_xgb_4_train)]\n",
        "ypredprob_geo_mean_test = [(aa*bb*cc*dd)**(1/num_models) for aa, bb, cc, dd in zip(ypred_xgb_1_test, ypred_xgb_2_test, ypred_xgb_3_test, ypred_xgb_4_test)]\n",
        "ypredprob_geo_mean_val = [(aa*bb*cc*dd)**(1/num_models) for aa, bb, cc, dd in zip(ypred_xgb_1_val, ypred_xgb_2_val, ypred_xgb_3_val, ypred_xgb_4_val)]\n",
        "ypred_geo_train = [1 if y >= 0.5 else 0 for y in ypredprob_geo_mean_train]\n",
        "ypred_geo_test = [1 if y >= 0.5 else 0 for y in ypredprob_geo_mean_test]\n",
        "ypred_geo_val = [1 if y >= 0.5 else 0 for y in ypredprob_geo_mean_val]\n",
        "\n",
        "\n",
        "# validate optimized for specificity\n",
        "THRESH = 0.785\n",
        "ypred_val_sp, ypred_prob_val_sp = [1 if y >= THRESH else 0 for y in ypredprob_geo_mean_val], [0.49 if (y >= 0.5 and y < THRESH) else y for y in ypredprob_geo_mean_val] \n",
        "\n",
        "\n",
        "# generate AUROC\n",
        "fpr_train, tpr_train, _ = roc_curve(y_train.values, ypredprob_geo_mean_train)\n",
        "fpr_test, tpr_test, _ = roc_curve(y_test.values, ypredprob_geo_mean_test)\n",
        "fpr_val, tpr_val, _ = roc_curve(y__test.values, ypredprob_geo_mean_val)\n",
        "fpr_valsp, tpr_valsp, _ = roc_curve(y__test.values, ypred_prob_val_sp)\n",
        "\n",
        "AUC_train = metrics.auc(fpr_train, tpr_train)\n",
        "AUC_test = metrics.auc(fpr_test, tpr_test)\n",
        "AUC_val = metrics.auc(fpr_val, tpr_val)\n",
        "AUC_valsp = metrics.auc(fpr_valsp, tpr_valsp)\n",
        "\n",
        "# Plot all ROC curves\n",
        "plt.figure()\n",
        "plt.plot(fpr_train, tpr_train, label= ' Train, AUC = %0.4f' % AUC_train)\n",
        "plt.plot(fpr_test, tpr_test, label= ' Test, AUC = %0.4f' % AUC_test)\n",
        "plt.plot(fpr_val, tpr_val, label= ' Validate, AUC = %0.4f' % AUC_val)\n",
        "plt.plot(fpr_valsp, tpr_valsp, label= f\" Validate, AUC = {AUC_valsp:0.4f} (specificity optimized)\")\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('1 - Specificity')\n",
        "plt.ylabel('Sensitivity')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# generate stats\n",
        "sn, sp, ppv, npv, pl, nl = [0 for i in range(4)] , [0 for i in range(4)], [0 for i in range(4)], [0 for i in range(4)], [0 for i in range(4)], [0 for i in range(4)]\n",
        "for i in range(0,4): \n",
        "  if i == 0:\n",
        "    txt = \"Train\"\n",
        "    cm = confusion_matrix(y_train, ypred_geo_train) \n",
        "    ModelReport(txt, y_train, ypred_geo_train, ypredprob_geo_mean_train)\n",
        "  if i == 1:\n",
        "    txt = \"Test\"\n",
        "    cm = confusion_matrix(y_test, ypred_geo_test) \n",
        "    ModelReport(txt, y_test, ypred_geo_test, ypredprob_geo_mean_test)\n",
        "  if i == 2:\n",
        "    txt = \"Validate\"\n",
        "    cm = confusion_matrix(y__test, ypred_geo_val) \n",
        "    ModelReport(txt, y__test, ypred_geo_val, ypredprob_geo_mean_val)\n",
        "  if i == 3:\n",
        "    txt = \"Validate (Spec optimized)\"\n",
        "    cm = confusion_matrix(y__test, ypred_val_sp) \n",
        "    ModelReport(txt, y__test, ypred_val_sp, ypred_prob_val_sp)\n",
        "  \n",
        "  sn[i] = cm[1,1]/(cm[1,1]+cm[1,0]) # Sens = TP / (TP + FN)\n",
        "  sp[i] = cm[0,0]/(cm[0,0]+cm[0,1]) # Spec = TN / (FP + TN)\n",
        "  ppv[i] = cm[1,1]/(cm[1,1]+cm[0,1]) # PPV = TP / (TP + FP)\n",
        "  npv[i] = cm[0,0]/(cm[0,0]+cm[1,0])# NPV = TN / (FN + TN)\n",
        "  pl[i] = sn[i]/(1-sp[i])\n",
        "  nl[i] = (1-sn[i])/sp[i]\n",
        "\n",
        "  print(f\"\\nCharacteristics, {txt}\")\n",
        "  print(f\"  sens: {sn[i]:0.4f}\")\n",
        "  print(f\"  spec: {sp[i]:0.4f}\")\n",
        "  print(f\"   ppv: {ppv[i]:0.4f}\")\n",
        "  print(f\"   npv: {npv[i]:0.4f}\")\n",
        "  print(f\"   LR+: {pl[i]:0.4f}\")\n",
        "  print(f\"   LR-: {nl[i]:0.4f}\")\n",
        "  print(f\"\\n\")\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}